:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

# Activity 1: Evaluating System Performance with GuideLLM

In Generative AI systems, evaluating system performance - including latency, throughput, and resource utilization - is just as important as evaluating model accuracy or quality. Here's why:

. **User Experience**: High latency leads to sluggish interactions, which is unacceptable in chatbots, copilots, and real-time applications. Users expect sub-second responses.

. **Scalability**: Throughput determines how many requests a system can handle in parallel. For enterprise GenAI apps, high throughput is essential to serve multiple users or integrate with high-frequency backend processes.

. **Cost Efficiency**: Slow or inefficient systems require more compute to serve the same volume of requests. Optimizing performance directly reduces cloud GPU costs and improves ROI.

. **Fair Benchmarking**: A model may appear “better” in isolated evaluation, but if it requires excessive inference time or hardware, it may not be viable in production. True model evaluation must balance quality and performance.

. **Deployment Readiness**: Latency and throughput impact architectural decisions (e.g., batching, caching, distributed serving). Measuring them ensures a model is viable under real-world constraints.

## Deploy GenAI Model with vLLM

Inference runtimes are the engines that turn a trained model into usable outputs. They handle the actual generation process during real-time use, so their efficiency directly affects latency, throughput, and resource usage. A poor runtime can bottleneck even the best model.

vLLM is an open-source, high-performance inference engine optimized for large language models. It uses advanced techniques like PagedAttention and continuous batching to significantly improve throughput and reduce latency—especially for workloads with many concurrent users. It's designed to maximize GPU utilization and support production-scale deployment.

### Export HuggingFace token 

Export your personal HuggingFace token into the active session.

NOTE: If you do not have a HF token, or do not know where to get one, head to https://huggingface.co/settings/tokens and create a new token. Keep it in a safe place, and don’t commit it to any git repositories.

[source,console,role=execute,subs=attributes+]
----
export HUGGING_FACE_HUB_TOKEN=<your actual huggingface token>
----

### Deploy vLLM container with model

[source,console,role=execute,subs=attributes+]
----
sudo podman run --rm -it --entrypoint /bin/sh -e HF_HOME=/tmp/model-cache -e HUGGING_FACE_HUB_TOKEN -e HF_HUB_OFFLINE=0 -v vllm-models:/tmp/model-cache --device nvidia.com/gpu=all -p 8000:8000/tcp quay.io/vllm/vllm:0.8.3.0rc0 -c 'vllm serve ibm-granite/granite-3.3-2b-instruct'
----

## Install GuideLLM

Activate preset python virtual environment located at `/home/dev` and then install **GuideLLM**:

[source,console,role=execute,subs=attributes+]
----
source venv/bin/activate
pip install guidellm
----

Explore GuideLLM configuration flags: 

* https://github.com/neuralmagic/guidellm?tab=readme-ov-file#configurations
* Use `guidellm benchmark --help` to list CLI options.

### Run Standard Performance Evaluation

Run a sweep test with GuideLLM using a reduced rate to keep runtime reasonable. The sweep will do numerous standard benchmarks at once. You may also adjust the rate-type to run one benchmark at a time.

[source,console,role=execute,subs=attributes+]
----
guidellm benchmark \
  --target http://localhost:8000 \
  --model ibm-granite/granite-3.3-2b-instruct \
  --rate-type sweep \
  --rate 5 \
  --max-seconds 30 \
  --data "prompt_tokens=256,output_tokens=128"
----

You can also define different token profiles per use case to simulate real-world traffic. 

For example, the following variables represent input and output tokens, respectively, based on the given use case: 

* **Chat (512/256)**
* **RAG (4096/512)**
* **Summarization (1024/256)**
* **Code Generation (512/512)**


### Evaluate Output and Adjust GuideLLM Settings

GuideLLM captures the following metrics during a full sweep:

. **Requests per Second**: Total requests completed per second

. **Request concurrency**: average concurrent requests

. **Output token per second (mean)**: output tokens per second

. **Total tokens per second (mean)**: total (prompt + output) tokens per second

. **Request latency in ms (mean, median, p99)**: total end to end request latency

. **Time to First Token (mean, median, p99)**

. **Inter-Token Latency (mean, median, p99)**

. **Time per output token (mean, median, p99)**

See the complete https://github.com/neuralmagic/guidellm/blob/main/docs/metrics.md[metrics documentation] for interpretation details. 

### Understanding Output Sections

#### Top Section (Benchmark Info)

* Benchmark: The type of benchmark ran
- constant@x indicates the number of requests sent constantly to the model per second.
* Requests Made: How many requests issued (completed, incomplete or errors)
* Token Data
- Tok/Req: average tokens per request
- Tok Total: total number of input/output tokens processed

#### Bottom Section (Benchmark Stats)

* Mean
- Overall average
- Good for general performance overview

* Median
- Represents typical experience
- More stable, less skewed by outliers

* P99
- Worst-case real latency
- Essential for SLOs and user experience under load

### Experiment with Different Scenarios

Try modifying input/output token configurations or test types to reflect:

* prompt_tokens=4096, output_tokens=512 (RAG)

* prompt_tokens=1024,output_tokens=256 (Summarization)

* prompt_tokens=512,output_tokens=512 (Code Generation)

## Summary

This activity demonstrated how to evaluate system performance using GuideLLM with a default vLLM configuration. In follow-up exercises, we would explore how to tune vLLM settings to optimize for different use cases and performance goals. 

For example:

* Adjusting batch size and max concurrency to improve throughput under load

* Tuning model cache size to reduce cold starts and improve latency

* Exploring speculative decoding and quantization for faster response times

By configuring vLLM more precisely or your chosen inference runtime, you can better align model serving with application needs—whether you’re optimizing for cost, speed, or user experience.
