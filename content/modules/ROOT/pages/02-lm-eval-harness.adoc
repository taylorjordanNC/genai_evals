:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

# Activity 2: Evaluating Model Accuracy with lm-eval-harness + MMLU-Pro

While performance metrics like latency and throughput are critical for deploying efficient GenAI systems, **task-level accuracy and reasoning quality** are equally essential for selecting or fine-tuning a model. In this activity, we use the popular lm-eval-harness framework to evaluate how well a language model performs across established benchmarks, focusing on reasoning and subject matter understanding.

## What is lm-eval-harness?

**lm-eval-harness** is a community-maintained benchmarking toolkit from **EleutherAI**. It enables consistent, reproducible evaluation of large language models (LLMs) across dozens of academic and real-world benchmarks, such as:

* MMLU (Massive Multitask Language Understanding)

* HellaSwag, ARC, and Winogrande

* Question answering, common sense reasoning, reading comprehension, and more

The framework supports both open-source models and OpenAI-compatible endpoints, and can be customized with additional tasks, prompt templates, and evaluation metrics.

## MMLU-Pro

Today we will be running the mmlu_pro evaluation. 

**MMLU-Pro** is a reasoning-focused, multiple-choice benchmark derived from the original MMLU dataset. MMLU-Pro extends the original MMLU benchmark by introducing 10-option multiple-choice questions across diverse academic disciplines. It’s designed to test a model’s **reasoning, factual recall, and elimination skills**—key for enterprise AI.

## Today's Activity

This activity uses a wrapper around lm-eval-harness and IBM’s Unitxt to make evaluation easier against locally hosted models via OpenAI-compatible inference endpoints like vLLM. It simplifies dataset downloads, task configuration, and results formatting—allowing you to quickly test your model on benchmarks like MMLU-Pro.

In this hands-on example, you’ll:

. Set up datasets using the CLI

. Run accuracy benchmarks against a vLLM-served model

. Explore how our model performs across standardized tasks

### Setup

Activate the same python virtual environment we used in the last activity, located in `/home/dev` and install the **llm-eval-test** package.

[source,console,role=execute,subs=attributes+]
----
source venv/bin/activate
pip install git+https://github.com/sjmonson/llm-eval-test.git
----

#### Check available CLI options

[source,console,role=execute,subs=attributes+]
----
llm-eval-test run --help
----

View our https://github.com/openshift-psap/llm-eval-test[repository] here.

### Prepare Dataset Directory

[source,console,role=execute,subs=attributes+]
----
DATASETS_DIR=$(pwd)/datasets
mkdir $DATASETS_DIR
DATASET=TIGER-Lab/MMLU-Pro
----

### Download Dataset

[source,console,role=execute,subs=attributes+]
----
llm-eval-test download --datasets $DATASETS_DIR --tasks mmlu_pro
----

### Define Your Model + Endpoint

We will use the Granite model still being served by vLLM. If this model is no longer deployed, please return to activity 1 to re-deploy the container.

[source,console,role=execute,subs=attributes+]
----
ENDPOINT=http://localhost:8000/v1/completions
MODEL_NAME=ibm-granite/granite-3.3-2b-instruct
TOKENIZER=ibm-granite/granite-3.3-2b-instruct
----

### Run the evaluation

[source,console,role=execute,subs=attributes+]
----
llm-eval-test run --endpoint $ENDPOINT --model $MODEL_NAME --datasets $DATASETS_DIR --tasks mmlu_pro
----

This may take ~10 minutes.

### While You Wait: What is lm-eval-harness?

* Check out this https://github.com/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb[overview notebook] to explore extensibility and task definitions.

* View real Red Hat https://huggingface.co/collections/RedHatAI/red-hat-ai-validated-models-v10-682613dc19c4a596dbac9437[validated model results] to understand benchmark outcomes in production contexts and to see how your favorite models rank.

## INterpreting MMLU-Pro Results

**Accuracy**: The primary metric is multiple-choice accuracy, indicating how often the model selects the correct answer from 10 options.

* ~10% = random guessing baseline

* ~30–50% = typical for smaller or untuned models

* ~60–70%+ = high reasoning capability or fine-tuned performance

**Per-subject Scores**: Breakdowns by subject (e.g., philosophy, law, computer science) help identify a model’s strengths and weaknesses in specific domains.

**Implications**: Higher MMLU-Pro accuracy generally correlates with better real-world task generalization, especially for tasks involving structured inputs, knowledge retrieval, and logic.

## Summary

This activity showed how to:

* Use llm-eval-harness via an OpenAI-compatible wrapper (llm-eval-test)

* Run benchmarks with MMLU-Pro

* Interpret accuracy-based metrics to guide model selection

Next steps might include:

* Comparing two models (e.g., open vs. fine-tuned)

* Adding custom tasks or subject domains

* Visualizing per-subject scores for insight into model gaps


